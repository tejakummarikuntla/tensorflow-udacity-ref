{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bridging Words with CharLSTMs",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "waJ7Ziogacn7",
        "Pawa3ZF6l0Ai",
        "rVxtf4FzldEa",
        "hK_LAJIMldEl",
        "aunQKOSJXixI",
        "go8f04-rldEz"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "waJ7Ziogacn7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Part 1 : Training a Name-Generating LSTM\n",
        "First, we need to train an LSTM on a large dataset of names, so it can generate artificial names by predicting the nth character given (n-1) characters of a name.\n"
      ]
    },
    {
      "metadata": {
        "id": "Pawa3ZF6l0Ai",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Imports and installs"
      ]
    },
    {
      "metadata": {
        "id": "LMwRtl9qldEQ",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Importing `pandas`, `numpy`, `random` and `sys`\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import sys"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "p9oQxQvhmYBo",
        "colab_type": "code",
        "outputId": "eff0a595-8885-4b61-d9ff-eb0cd2a2bf24",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "cell_type": "code",
      "source": [
        "#@title Downloading dataset :  Baby Names from Social Security Card Applications - National Level Data \n",
        "!wget https://raw.githubusercontent.com/jcbain/celeb_baby_names/master/data/NationalNames.csv"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2018-12-08 12:10:16--  https://raw.githubusercontent.com/jcbain/celeb_baby_names/master/data/NationalNames.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 44350518 (42M) [text/plain]\n",
            "Saving to: ‘NationalNames.csv’\n",
            "\n",
            "NationalNames.csv   100%[===================>]  42.30M  39.0MB/s    in 1.1s    \n",
            "\n",
            "2018-12-08 12:10:22 (39.0 MB/s) - ‘NationalNames.csv’ saved [44350518/44350518]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qQ9baW0KbjeQ",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Ignore warnings in output\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rVxtf4FzldEa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Loading and pre-processing data. "
      ]
    },
    {
      "metadata": {
        "id": "yrrLkLYrQDcN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "`SEQLEN` is the number of characters our LSTM uses to predict the next character</br>\n",
        "`STEP` number of letters to skip between two samples\n",
        "\n",
        "Hence, the name `PETER` is used to generate the following samples :\n",
        "\n",
        "| X1  |  X2  | X3  | Y  |\n",
        "|--:|---|---|---|\n",
        "| -  | - | P  | **E**  |\n",
        "| -  | P | E  | **T**  |\n",
        "| P | E | T  | **E**  |\n",
        "| E  | T | E  | **R**  |\n",
        "| T  | E | R  | **-**  |"
      ]
    },
    {
      "metadata": {
        "id": "THqKsNfLPzG7",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Declaring `SEQLEN` and `STEP`\n",
        "SEQLEN = 3\n",
        "STEP = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7tmcfBSwS8jJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "- Loading names from `NationalNames.csv`\n",
        "- Eliminating names shorter than 4 chars and having frequency less than 3\n",
        "- Joining (seperating) names with `\\n`\n",
        "\n",
        "`Loaded 87659 names with 27 characters.`\n"
      ]
    },
    {
      "metadata": {
        "id": "FQQDc_w2T9q3",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title get_data\n",
        "def get_data():\n",
        "    df = pd.read_csv('NationalNames.csv')\n",
        "    names = list(df[(df['Count'] > 3) & (df['Name'].str.len() > 4)]['Name'].unique())\n",
        "    text = '\\n' + '\\n\\n'.join(names).lower() + '\\n'\n",
        "    chars = sorted(list(set(text)))\n",
        "\n",
        "    print (\"Loaded\",len(names),\"names with\",len(chars),\"characters.\")\n",
        "    return text,chars"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0eNtEstnUhz5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "- Splitting text into `sequences` of 3 characters (X) and adding next character to `next_chars` (y)\n",
        "\n",
        "```\n",
        "No. of sequences: 764939\n",
        "No. of chars: 764939\n",
        "```"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "6XbLPs6lUT9x",
        "cellView": "both",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title get_seq\n",
        "def get_seq(args):\n",
        "    text = args[0]\n",
        "    sequences = []\n",
        "    next_chars = []\n",
        "    for i in range(0, len(text) - SEQLEN, STEP):\n",
        "        sequences.append(text[i: i + SEQLEN])\n",
        "        next_chars.append(text[i + SEQLEN])\n",
        "    print('No. of sequences:', len(sequences))\n",
        "    print('No. of chars:', len(next_chars))\n",
        "    return sequences,next_chars,args[1]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nbOjgQeYT-b8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "- One-Hot Encoding characters in `sequences` and `next_chars`\n",
        "\n",
        "```\n",
        "Shape of X (sequences): (764939, 3, 27)\n",
        "Shape of y (next_chars): (764939, 27)\n",
        "```"
      ]
    },
    {
      "metadata": {
        "id": "BjhcYQi8ldEc",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title get_vectors\n",
        "def one_hot(word,char_indices):\n",
        "    x_pred = np.zeros((1, SEQLEN, 27))\n",
        "\n",
        "    for t, char in enumerate(word):\n",
        "        x_pred[0, t, char_indices[char]] = 1.\n",
        "    return x_pred\n",
        "  \n",
        "def get_vectors(args):\n",
        "    sequences,next_chars,chars = args\n",
        "    char_indices = dict((c, i) for i, c in enumerate(chars))\n",
        "    indices_char = dict((i, c) for i, c in enumerate(chars))\n",
        "    X = np.zeros((len(sequences), SEQLEN, len(chars)), dtype=np.bool)\n",
        "    y = np.zeros((len(sequences), len(chars)), dtype=np.bool)\n",
        "    for i, sentence in enumerate(sequences):\n",
        "        X[i] = one_hot(sentence,char_indices)\n",
        "        y[i, char_indices[next_chars[i]]] = 1\n",
        "    print (\"Shape of X (sequences):\",X.shape)\n",
        "    print (\"Shape of y (next_chars):\",y.shape)\n",
        "    return X,y,char_indices, indices_char"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hK_LAJIMldEl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Creating the LSTM Model\n",
        "- We're creating a simple LSTM model that takes in a sequence of size `SEQLEN`, each element of `len(chars)` numbers (1 or 0)\n",
        "- The output of the LSTM goes into a Dense layer that predicts the next character with a softmaxed one-hot encoding\n",
        "\n",
        "Note: While running on Google Colab, choose the TPU for training <br>(`Edit > Notebook settings > Hardware Accelerator > TPU`)"
      ]
    },
    {
      "metadata": {
        "id": "xiDLJporldEo",
        "colab_type": "code",
        "cellView": "both",
        "outputId": "db5371e3-5baf-43c7-cdd2-6a8a146c2523",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "#@title Defining our model with Keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation\n",
        "from keras.layers import LSTM\n",
        "from keras.optimizers import RMSprop\n",
        "\n",
        "def get_model(num_chars):\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(16, input_shape=(SEQLEN, num_chars)))\n",
        "    model.add(Dense(num_chars))\n",
        "    model.add(Activation('softmax'))\n",
        "\n",
        "    optimizer = RMSprop(lr=0.01)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "nWjmw9aqZpxi",
        "colab_type": "code",
        "cellView": "both",
        "outputId": "4e2eef40-0369-42c9-a1ab-e7289e9b114c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "cell_type": "code",
      "source": [
        "#@title Loading the data and declaring our model\n",
        "X,y,char_indices, indices_char = get_vectors(get_seq(get_data()))\n",
        "model = get_model(len(char_indices.keys()))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded 87659 names with 27 characters.\n",
            "No. of sequences: 764939\n",
            "No. of chars: 764939\n",
            "Shape of X (sequences): (764939, 3, 27)\n",
            "Shape of y (next_chars): (764939, 27)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "gTG8OBl0ldEv",
        "colab_type": "code",
        "outputId": "9f985afc-625e-4912-992e-20100df777bd",
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 863
        }
      },
      "cell_type": "code",
      "source": [
        "#@title Training the model for 10 epochs\n",
        "\n",
        "# On Google Colab, each epoch should take about 27 seconds\n",
        "# For better results, you can run this cell again (and again)\n",
        "model.fit(X, y,\n",
        "          batch_size=128,\n",
        "          epochs=40)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "  4608/764939 [..............................] - ETA: 3:15 - loss: 2.7143"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-39cba69772a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m model.fit(X, y,\n\u001b[1;32m      2\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m           epochs=40)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "aunQKOSJXixI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Loading Pre-trained CharLSTM model\n",
        "\n",
        "Alternatively you can also load a pretrained model"
      ]
    },
    {
      "metadata": {
        "id": "YZ87xV4aW7L_",
        "colab_type": "code",
        "cellView": "form",
        "outputId": "8d3a54d6-655a-41c5-f1b9-3b0c5488b8f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "#@title Downloading model and weight files\n",
        "!wget https://github.com/py-ranoid/WhatDoWeCallIt/raw/master/model_keras.h5 -nv\n",
        "!wget https://github.com/py-ranoid/WhatDoWeCallIt/raw/master/model_keras.json -nv\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2018-12-08 12:11:42 URL:https://raw.githubusercontent.com/py-ranoid/WhatDoWeCallIt/master/model_keras.h5 [25452/25452] -> \"model_keras.h5\" [1]\n",
            "2018-12-08 12:11:44 URL:https://raw.githubusercontent.com/py-ranoid/WhatDoWeCallIt/master/model_keras.json [1582/1582] -> \"model_keras.json\" [1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4jpe4993Xvxb",
        "colab_type": "code",
        "cellView": "both",
        "outputId": "46de791a-ac64-4b83-a56d-d12bbe067776",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "cell_type": "code",
      "source": [
        "X,y,char_indices, indices_char = get_vectors(get_seq(get_data()))\n",
        "\n",
        "#@title Load Pretained Model (optional, if model has been saved)\n",
        "from keras.models import model_from_json\n",
        "\n",
        "try:\n",
        "  json_file = open('model_keras.json', 'r')\n",
        "  loaded_model_json = json_file.read()\n",
        "  json_file.close()\n",
        "  model = model_from_json(loaded_model_json)\n",
        "  model.load_weights(\"model_keras.h5\")\n",
        "  print(\"Loaded model from disk\")  \n",
        "  \n",
        "except FileNotFoundError:\n",
        "  print (\"Models not found. Train them, maybe ?\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded 87659 names with 27 characters.\n",
            "No. of sequences: 764939\n",
            "No. of chars: 764939\n",
            "Shape of X (sequences): (764939, 3, 27)\n",
            "Shape of y (next_chars): (764939, 27)\n",
            "Loaded model from disk\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "go8f04-rldEz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Sampling with our model\n",
        "- Picking the element with the greatest probability will always return the same character for a given sequence\n",
        "- I'd like to induce some variance by sampling from a probability array instead.\n",
        "\n",
        "To explain this better, here's an excerpt  from Andrej Karpathy's blog aobut CharRNNs : \n",
        "> Temperature. We can also play with the temperature of the Softmax during sampling. Decreasing the **temperature from 1 to some lower number (e.g. 0.5) makes the RNN more confident, but also more conservative** in its samples. Conversely, **higher temperatures will give more diversity but at cost of more mistakes** (e.g. spelling mistakes, etc). In particular, setting temperature very near zero will give the most likely thing that Paul Graham might say:\n",
        "\n",
        "> *“is that they were all the same thing that was a startup is that they were all the same thing that was a startup is that they were all the same thing that was a startup is that they were all the same”*\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "68DpYpH6ldE1",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Sampling and Generating functions\n",
        "def sample(preds, temperature=1.0):\n",
        "    # helper function to sample an index from a probability array\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)\n",
        "\n",
        "def gen_name(seed):\n",
        "    generated = seed\n",
        "    for i in range(10):\n",
        "        x_pred = np.zeros((1, SEQLEN, 27))\n",
        "        for t, char in enumerate(seed):\n",
        "            x_pred[0, t, char_indices[char]] = 1.\n",
        "        preds = model.predict(x_pred, verbose=0)[0]\n",
        "        next_char = indices_char[sample(preds,0.5)]\n",
        "        if next_char == '\\n':break\n",
        "        generated += next_char\n",
        "        seed = seed[1:] + next_char\n",
        "\n",
        "    return generated"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3pkOf88DldE6",
        "colab_type": "code",
        "outputId": "29631299-f4f3-4357-92f1-e7879e7589bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "cell_type": "code",
      "source": [
        "for i in ['dav','ram','seb']:\n",
        "    print ('Seed: \"'+i+'\"\\tNames :',[gen_name(i) for _ in range(5)]) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Seed: \"dav\"\tNames : ['davia', 'davia', 'davia', 'davetta', 'davia']\n",
            "Seed: \"ram\"\tNames : ['ram', 'ramia', 'ramiriann', 'ramir', 'ramari']\n",
            "Seed: \"seb\"\tNames : ['seb', 'seberta', 'seberta', 'seberniann', 'seboria']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6jWkqG9qa1Jt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Part 2 : Use a CharLSTM model to join words"
      ]
    },
    {
      "metadata": {
        "id": "aLcDEcuYfiG8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "- `sample_preds` : Probabilities of 27 characters (A-Z + \\n) to follow given sequence\n",
        "- `ohmygauss`:Function to return decreasing gaussian sequence (right half of bell curve)"
      ]
    },
    {
      "metadata": {
        "id": "2DnIgVVRgIrY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from scipy.stats import norm\n",
        "def sample_preds(preds, temperature=1.0):\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    return preds\n",
        "\n",
        "\n",
        "def ohmygauss(length, sigma=1.8):\n",
        "    rv = norm(loc=0, scale=sigma)\n",
        "    x = np.arange(length)\n",
        "    return rv.pdf(x)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "i0FRWCFdgJYf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Getting bridge scores\n",
        "1. Iterate over all sequences of 3 (`SEQLEN`) characters in left word. (`MINLEFT -> n`)\n",
        "  1. Iterate over all sequences of 3 (`COMPARE`) characters in right word. (`0 -> MINRIGHT`)\n",
        "    1. Get probability that given character in right word will follow sequence from word\n",
        "    2. Repeat for `COMPARE` sequences.</br>\n",
        "    For example : to bridge **britain** and **exit** at `_br+exit`, <br>\n",
        "     Score : `prob(e|\"_br\")*w1 + prob(x|\"bre\")*w2 + prob(i|\"rex\")*w3`\n",
        "     3. Multiply Gaussian factors to score to prioritize words that are bridges towards the beggining of the right word"
      ]
    },
    {
      "metadata": {
        "id": "CB_uILFaldFW",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Define `proc` to get bridge scores using LSTMs\n",
        "MINLEFT = 3\n",
        "MINRIGHT = 3\n",
        "COMPARE = 3\n",
        "LEFT_BIAS = [0.06, 0.05, 0.04]\n",
        "\n",
        "def proc(left, right, verbose=False):\n",
        "    best_matches = {}\n",
        "    best_i = None\n",
        "    best_i_score = -1\n",
        "    for i in range(0, len(left) - MINLEFT + 1):\n",
        "        # Searching all sequences of size COMPARE in the right word\n",
        "        # to find best match\n",
        "        best_j = None\n",
        "        best_j_score = -1\n",
        "        best_matches[i] = {}\n",
        "        right_bound = len(right) - MINRIGHT + 1\n",
        "        gaus_factors = ohmygauss(right_bound)\n",
        "        for j in range(0, right_bound):\n",
        "            right_chars = right[j:j + COMPARE]\n",
        "            s = 0\n",
        "            for x in range(COMPARE):\n",
        "                \n",
        "                # Character on right which is being sampled\n",
        "                c_index = char_indices[right_chars[x]]\n",
        "                if verbose:\n",
        "                    print (\"Sampling \" + left[i + x:i + SEQLEN] +\n",
        "                           right[j:j + x] + \"-->\" + right_chars[x])\n",
        "\n",
        "                # Generating sequence and getting probability\n",
        "                Xoh = one_hot(left[i + x:i + SEQLEN] + right[j:j + x],char_indices)\n",
        "                preds = model.predict(Xoh, verbose=0)[0]\n",
        "                pred_probs = sample_preds(preds, 0.7)\n",
        "\n",
        "                # Getting corresponding character in left word\n",
        "                left_char = np.zeros((1, len(char_indices)))\n",
        "                try:\n",
        "                    left_char[0, char_indices[left[i + SEQLEN + x]]] = 1\n",
        "                except IndexError:\n",
        "                    pass\n",
        "                # Adding some bias to left_char and adding it to predicted probs\n",
        "                biased_probs = LEFT_BIAS[x] * left_char + \\\n",
        "                    (1 - LEFT_BIAS[x]) * pred_probs\n",
        "\n",
        "                # Adding probability of bridging at c_index to s\n",
        "                s += biased_probs[0, c_index]\n",
        "\n",
        "            # Prioritizing words that start with the first few letters of the right word\n",
        "            s = s * gaus_factors[j]\n",
        "\n",
        "            if verbose:\n",
        "                print (i, j, s,)\n",
        "            best_matches[i][j] = s\n",
        "            if s > best_j_score:\n",
        "                best_j = j\n",
        "                best_j_score = s\n",
        "#         best_matches[i] = {'index': best_j, 'score': best_j_score}\n",
        "        if best_j_score > best_i_score and i < len(left) - MINLEFT:\n",
        "            best_i_score = best_j_score\n",
        "            best_i = i\n",
        "\n",
        "    return best_matches, best_i"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "45_BJReElXfc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Picking the best portmanteaus\n",
        "\n",
        "- Maximize smoothness of the bridge (derived from `proc` using the LSTM  model)\n",
        "- Minimize length of portmanteau\n",
        "- Maximize fraction of each word in portmanteau"
      ]
    },
    {
      "metadata": {
        "id": "9r-qWTTCaqoM",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Define `join` to pick best portmanteaus\n",
        "\n",
        "SEQLEN = 3\n",
        "MAXLEN = 10\n",
        "PHONEME_WT = 4\n",
        "\n",
        "def join(left, right, verbose=False,dict_result=False,n=3):\n",
        "    left = '\\n' + left\n",
        "    right = right + '\\n'\n",
        "    matches, i = proc(left, right, verbose)\n",
        "    probs = {}\n",
        "    for i_temp in matches:\n",
        "        for j_temp in matches[i_temp]:\n",
        "            word = (left[:i_temp + SEQLEN] + right[j_temp:]).replace('\\n', '').title()\n",
        "            num_letters = len(word)\n",
        "            if verbose :\n",
        "                print (word, num_letters,(1 / float(num_letters)) * 0.5)\n",
        "            probs[word] = probs.get(word,0)+round(matches[i_temp][j_temp],4) + (1 / float(num_letters) * PHONEME_WT)\n",
        "            probs[word] *= (min((i_temp+1)/min(len(left),8),1.0) + min((len(right) - j_temp - 1)/min(len(right),8),1.0))\n",
        "    if dict_result:\n",
        "        return probs\n",
        "    else:\n",
        "        ser = pd.Series(probs).sort_values()[::-1][:n]\n",
        "        ports = ser.index.tolist()\n",
        "        port_vals = [i+'('+str(round(ser[i],3))+')' for i in ports]\n",
        "        print (left,'+',right,' = ',port_vals)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ToyG6N-Ub6bO",
        "colab_type": "code",
        "outputId": "70e8ab2d-253e-448e-e391-afea7ca11f26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486
        }
      },
      "cell_type": "code",
      "source": [
        "pokemon_pairs = [('char','lizard'), ('venus', 'dinosaur'), ('blast', 'tortoise'), ('pikapika', 'chu')]        \n",
        "word_pairs =  [('britain','exit'),('biology', 'electronic'), ('affluence', 'influenza'), ('brad', 'angelina'),\n",
        "               ('brother', 'romance'), ('breakfast', 'lunch'), ('chill', 'relax'), ('emotion', 'icon'),('feminist', 'nazi')]\n",
        "\n",
        "for p in word_pairs:\n",
        "  join(p[0],p[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "britain + exit\n",
            "  =  ['Britainexit(0.71)', 'Brexit(0.705)', 'Briexit(0.69)']\n",
            "\n",
            "biology + electronic\n",
            "  =  ['Biolectronic(1.23)', 'Biolonic(0.821)', 'Bionic(0.677)']\n",
            "\n",
            "affluence + influenza\n",
            "  =  ['Affluenza(2.722)', 'Affluenfluenza(1.261)', 'Affluencenza(1.093)']\n",
            "\n",
            "brad + angelina\n",
            "  =  ['Brangelina(1.626)', 'Braangelina(0.637)', 'Bradangelina(0.635)']\n",
            "\n",
            "brother + romance\n",
            "  =  ['Brotheromance(1.493)', 'Bromance(0.963)', 'Brothermance(0.625)']\n",
            "\n",
            "breakfast + lunch\n",
            "  =  ['Breaunch(0.657)', 'Breakfasunch(0.59)', 'Breakfalunch(0.588)']\n",
            "\n",
            "chill + relax\n",
            "  =  ['Chillax(1.224)', 'Chilax(1.048)', 'Chillelax(0.699)']\n",
            "\n",
            "emotion + icon\n",
            "  =  ['Emoticon(1.331)', 'Emotion(0.69)', 'Emicon(0.667)']\n",
            "\n",
            "feminist + nazi\n",
            "  =  ['Feminazi(1.418)', 'Femazi(0.738)', 'Feministazi(0.678)']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "T3BXWBZtnyUR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Result"
      ]
    },
    {
      "metadata": {
        "id": "VlT85N4OnCg6",
        "colab_type": "code",
        "cellView": "form",
        "outputId": "71b0f9c2-cddb-4405-cb3e-0424d70ad605",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "cell_type": "code",
      "source": [
        "#@title Demo Time!\n",
        "\n",
        "left = 'chocolate' #@param {type:\"string\"}\n",
        "right = 'alcoholic' #@param {type:\"string\"}\n",
        "num_portmanteaus = 2 #@param {type:\"slider\", min:0, max:10, step:1}\n",
        "join(left,right,n=num_portmanteaus)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "chocolate + alcoholic\n",
            "  =  ['Chocoholic(1.558)', 'Chocolalcoholic(1.351)']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}